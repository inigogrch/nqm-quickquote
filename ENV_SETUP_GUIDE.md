# Environment Variables Setup Guide

## ğŸ“‹ Typical Production Setup

For a production deployment where conditions results go to the **same S3 bucket** as document uploads:

```bash
# AWS S3 Configuration (uploads & results)
VITE_AWS_ACCESS_KEY_ID=AKIA...
VITE_AWS_SECRET_ACCESS_KEY=xxxxx...
VITE_AWS_S3_BUCKET=quick-quote-demo
VITE_AWS_REGION=us-east-1

# Rack Stack API (conditions check)
VITE_RACK_STACK_USERNAME=airflow
VITE_RACK_STACK_PASSWORD=xxxxx...
VITE_RACK_STACK_OUTPUT_BUCKET=quick-quote-demo  # â† Same as S3 bucket
VITE_RACK_STACK_OUTPUT_PREFIX=results/          # Optional, defaults to 'results/'

# Supabase (optional)
VITE_SUPABASE_URL=https://xxx.supabase.co
VITE_SUPABASE_ANON_KEY=xxxxx...
```

## ğŸ—‚ï¸ How Files Are Organized

With the above configuration, your S3 bucket structure will be:

```
s3://quick-quote-demo/
  â”œâ”€â”€ documents/                      # â† PDFs uploaded by users
  â”‚   â””â”€â”€ {programId}/
  â”‚       â””â”€â”€ condition_{conditionId}/
  â”‚           â””â”€â”€ {timestamp}_document.pdf
  â”‚
  â””â”€â”€ results/                        # â† JSON results from Airflow
      â””â”€â”€ conditions_{timestamp}.json
```

## âœ… What This Means

1. **Document Upload**: Users upload PDFs â†’ saved to `s3://quick-quote-demo/documents/...`
2. **Conditions API**: Triggered with those S3 paths â†’ Airflow processes them
3. **Results**: Airflow writes JSON to `s3://quick-quote-demo/results/conditions_*.json`
4. **Frontend Polling**: Reads JSON from S3 (NOT hardcoded!) and displays results

## ğŸ” Verification

After setting environment variables, check your browser console. You should see:

```javascript
ğŸ“‹ Configuration Status: {
  s3: {
    configured: true,
    bucket: 'âœ… quick-quote-demo',
    ...
  },
  rackStack: {
    configured: true,
    outputBucket: 'âœ… quick-quote-demo',
    outputPrefix: 'results/',
    ...
  },
  bucketSetup: 'âœ… Using same bucket: quick-quote-demo'
}
```

## ğŸ’¡ Alternative Setup (Separate Buckets)

If you need to use **different buckets** for uploads vs results:

```bash
VITE_AWS_S3_BUCKET=upload-bucket           # For document uploads
VITE_RACK_STACK_OUTPUT_BUCKET=results-bucket  # For API results
```

The console will show:
```
âš ï¸ Different buckets - S3: upload-bucket / Output: results-bucket
```

This is supported but requires your AWS credentials to have access to both buckets.

## ğŸš« Common Issues

### Issue: "File not found in S3"
**Solution**: The Airflow DAG is still processing. Polling continues automatically every 10 seconds until results are ready.

### Issue: "S3 credentials not configured"
**Solution**: Make sure all `VITE_AWS_*` variables are set and restart your dev server.

### Issue: "Rack Stack not configured"
**Solution**: Set `VITE_RACK_STACK_OUTPUT_BUCKET` - it's required even if it's the same as your S3 bucket.

## ğŸ“ Code References

- **Configuration**: `src/lib/config/credentials.ts`
- **Conditions API**: `src/lib/api/conditions.ts`
- **Upload Logic**: `components/submission/ConditionsSection.tsx` (lines 90-143)
- **Results Polling**: `src/lib/api/conditions.ts` (lines 159-205)

All paths are **real S3 paths** - nothing is hardcoded! ğŸ‰

